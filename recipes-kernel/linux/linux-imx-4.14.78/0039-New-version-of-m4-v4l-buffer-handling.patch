From 2a5e8486e783d62e67be979b2981de2a7e13e587 Mon Sep 17 00:00:00 2001
From: David Sernelius <david.sernelius@flir.se>
Date: Wed, 8 May 2019 11:51:37 +0200
Subject: [PATCH] New version of m4 <-> v4l buffer handling

---
 .../media/platform/mxc/capture/mxc_rpmsg.c    | 172 +++++++++---------
 1 file changed, 87 insertions(+), 85 deletions(-)

diff --git a/drivers/media/platform/mxc/capture/mxc_rpmsg.c b/drivers/media/platform/mxc/capture/mxc_rpmsg.c
index f89b87746884..2d7879c78523 100644
--- a/drivers/media/platform/mxc/capture/mxc_rpmsg.c
+++ b/drivers/media/platform/mxc/capture/mxc_rpmsg.c
@@ -75,7 +75,6 @@ struct imx_rpmsg_fmt {
 
 struct rpmsg_buf_internal {
 	struct list_head queue;
-	bool discard;
 };
 
 struct imx_rpmsg_buffer {
@@ -93,14 +92,9 @@ struct imx_rpmsg_device {
 
 	spinlock_t slock;
 	struct list_head active_queue;
-	struct list_head discard;
 	uint32_t frame_count;
 	bool streaming;
 
-	void *discard_buffer;
-	dma_addr_t discard_buffer_dma;
-	size_t discard_size;
-	struct rpmsg_buf_internal	buf_discard;
 	struct mutex lock;
 	atomic_t use_count;
 	unsigned int curr_input;
@@ -587,61 +581,46 @@ static int imx_rpmsg_buf_prepare(struct vb2_buffer *vb)
 }
 
 static int imx_rpmsg_start_streaming(struct vb2_queue *q,
-				   unsigned int count)
+		unsigned int count)
 {
 	int ret = 0;
 	unsigned long flags;
 	struct imx_rpmsg_device *rpmsg_dev = vb2_get_drv_priv(q);
-	struct imx_rpmsg_buffer *rpmsg_buf;
+	struct imx_rpmsg_buffer *buf, *tmp;
 	struct vb2_buffer *vb;
-	dma_addr_t dma_addr;
+	int d = 0, i = 0;
+	dma_addr_t dma_addr[16] = {0};
 
-	if (WARN_ON(count < 3))
+	if (WARN_ON(count < 3) || WARN_ON(count > 16))
 		return -ENOBUFS;
 
-	/* discard buffers for no buffer available */
-	rpmsg_dev->discard_size = rpmsg_dev->v4l2_pix_fmt.sizeimage;
-	rpmsg_dev->discard_buffer = dma_alloc_coherent(rpmsg_dev->v4l2_dev.dev,
-					PAGE_ALIGN(rpmsg_dev->discard_size),
-					&rpmsg_dev->discard_buffer_dma,
-					GFP_DMA | GFP_KERNEL);
-	if (!rpmsg_dev->discard_buffer)
-		return -ENOMEM;
-
 	spin_lock_irqsave(&rpmsg_dev->slock, flags);
 
-	/* queue the discard buffer first */
-	rpmsg_dev->buf_discard.discard = true;
-	list_add_tail(&rpmsg_dev->buf_discard.queue,
-		      &rpmsg_dev->discard);
-
 	if (unlikely(list_empty(&rpmsg_dev->active_queue))) {
 		WARN_ON(1);
 		spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
 		return -ENOBUFS;
 	}
 
-	rpmsg_buf = list_first_entry(&rpmsg_dev->active_queue,
-				   struct imx_rpmsg_buffer,
-				   internal.queue);
-
-	vb = &rpmsg_buf->vb.vb2_buf;
-	dma_addr = vb2_dma_contig_plane_dma_addr(vb, 0);
-
-	rpmsg_buf->field = 0;
+	list_for_each_entry_safe(buf, tmp,
+			&rpmsg_dev->active_queue, internal.queue) {
+		vb = &buf->vb.vb2_buf;
+		vb->state = VB2_BUF_STATE_ACTIVE;
+		dma_addr[d++] = vb2_dma_contig_plane_dma_addr(vb, 0);
+		buf->field = 0;
+	}
 
 	spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
 
 	ret = imx_rpmsg_set_fmt(rpmsg_dev);
 	if (ret)
-		return ret;
+        return ret;
 
 	rpmsg_dev->streaming = true;
 
 	/* enable dma transfer */
-	ret = imx_rpmsg_config_dma(rpmsg_dev, dma_addr, 0);
-	if (ret)
-		return ret;
+	for (i = 0; i < d && ret == 0; ++i)
+		ret = imx_rpmsg_config_dma(rpmsg_dev, dma_addr[i], 0);
 
 	return ret;
 }
@@ -671,16 +650,8 @@ static void imx_rpmsg_stop_streaming(struct vb2_queue *q)
 	}
 
 	INIT_LIST_HEAD(&rpmsg_dev->active_queue);
-	INIT_LIST_HEAD(&rpmsg_dev->discard);
-
-	tmpbuf = rpmsg_dev->discard_buffer;
-	rpmsg_dev->discard_buffer = NULL;
 
 	spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
-
-	dma_free_coherent(rpmsg_dev->v4l2_dev.dev,
-				rpmsg_dev->discard_size, tmpbuf,
-				rpmsg_dev->discard_buffer_dma);
 }
 
 static void imx_rpmsg_buf_queue(struct vb2_buffer *vb)
@@ -691,6 +662,7 @@ static void imx_rpmsg_buf_queue(struct vb2_buffer *vb)
 						struct imx_rpmsg_buffer,
 						vb);
 	unsigned long flags;
+    dma_addr_t dma_addr;
 
 	spin_lock_irqsave(&rpmsg_dev->slock, flags);
 
@@ -701,6 +673,13 @@ static void imx_rpmsg_buf_queue(struct vb2_buffer *vb)
 	list_add_tail(&rpmsg_buf->internal.queue, &rpmsg_dev->active_queue);
 
 	spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
+
+	if (rpmsg_dev->streaming) {
+		vb->state = VB2_BUF_STATE_ACTIVE;
+		dma_addr = vb2_dma_contig_plane_dma_addr(vb, 0);
+		imx_rpmsg_config_dma(rpmsg_dev, dma_addr, 0);
+	}
+
 }
 
 static struct vb2_ops imx_rpmsg_vb2_ops = {
@@ -713,15 +692,20 @@ static struct vb2_ops imx_rpmsg_vb2_ops = {
 	.buf_queue		= imx_rpmsg_buf_queue,
 };
 
-static int imx_rpmsg_dma_done_handle(struct imx_rpmsg_device *rpmsg_dev)
+static int imx_rpmsg_dma_done_handle(struct imx_rpmsg_device *rpmsg_dev, dma_addr_t dma_addr)
 {
-	int ret;
-	uint32_t field = 0;
+#if 0 	/* Still problems with what seems to be "double reporting" of done dma transfers, use approach with no checks instead.
+		/ TODO: this should be investigated more */
+	bool found = false;
 	unsigned long flags;
-	struct imx_rpmsg_buffer *buf;
-	struct rpmsg_buf_internal *ibuf;
+	struct imx_rpmsg_buffer *buf, *tmp;
 	struct vb2_buffer *vb;
-	dma_addr_t dma_addr;
+    static dma_addr_t last_dma = 0;
+    if (dma_addr == last_dma) {
+		dev_err(rpmsg_dev->dev, "Same dma addr: 0x%X as previous reported done.", dma_addr);
+    }
+
+    last_dma = dma_addr;
 
 	spin_lock_irqsave(&rpmsg_dev->slock, flags);
 
@@ -731,62 +715,81 @@ static int imx_rpmsg_dma_done_handle(struct imx_rpmsg_device *rpmsg_dev)
 		return -ENOBUFS;
 	}
 
-	ibuf = list_first_entry(&rpmsg_dev->active_queue, struct rpmsg_buf_internal,
-				queue);
-
-	if (ibuf->discard) {
-
-		/* discard buffer just return to discard queue */
-		/* not dqbuf to user */
-		list_move_tail(rpmsg_dev->active_queue.next, &rpmsg_dev->discard);
-	} else {
-		/* make vb2 know about the buffer done, can return to user */
-		buf = rpmsg_ibuf_to_buf(ibuf);
+	list_for_each_entry_safe(buf, tmp,
+			&rpmsg_dev->active_queue, internal.queue) {
 		vb = &buf->vb.vb2_buf;
+		if (vb2_dma_contig_plane_dma_addr(vb, 0) == dma_addr) {
+			/* delete current buffer from queue  and make vb2 know buffer is done. */
+			list_del_init(&buf->internal.queue);
+			to_vb2_v4l2_buffer(vb)->sequence = rpmsg_dev->frame_count++;
+			vb2_buffer_done(vb, VB2_BUF_STATE_DONE);
+			found = true;
+			break;
+		}
+	}
+
+	if (!found) {
+        dma_addr_t dm[16] = {0};
+        int d = 0, i = 0;
+        list_for_each_entry_safe(buf, tmp,
+                &rpmsg_dev->active_queue, internal.queue) {
+			vb = &buf->vb.vb2_buf;
+			dm[d++] = vb2_dma_contig_plane_dma_addr(vb, 0);
+			if (d >= 16) break;
+        }
+		spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
 
-		/* delete current buffer from queue */
-		list_del_init(&buf->internal.queue);
-		to_vb2_v4l2_buffer(vb)->sequence = rpmsg_dev->frame_count;
-		vb2_buffer_done(vb, VB2_BUF_STATE_DONE);
+		dev_err(rpmsg_dev->dev, "Couldn't find matching vb2_buf for dma addr: 0x%X "
+				"[ 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X 0x%X ]", dma_addr,
+				dm[0], dm[1], dm[2], dm[3], dm[4], dm[5], dm[6], dm[7],
+				dm[8], dm[9], dm[10], dm[11], dm[12], dm[13], dm[14], dm[15]);
+		return 0;
 	}
 
-	rpmsg_dev->frame_count++;
+	spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
+#endif
 
-	if (list_empty(&rpmsg_dev->active_queue)) {
+	unsigned long flags;
+	struct imx_rpmsg_buffer *buf;
+	struct vb2_buffer *vb;
 
-		if (list_empty(&rpmsg_dev->discard)) {
-			spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
-			dev_err(rpmsg_dev->dev,
-				"trying to access empty discard list\n");
-			return 0;
-		}
+	spin_lock_irqsave(&rpmsg_dev->slock, flags);
 
-		list_move_tail(rpmsg_dev->discard.next, &rpmsg_dev->active_queue);
+	if (unlikely(list_empty(&rpmsg_dev->active_queue))) {
+		WARN_ON(1);
 		spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
+		return -ENOBUFS;
+	}
 
-		imx_rpmsg_config_dma(rpmsg_dev, rpmsg_dev->discard_buffer_dma, 0);
+	buf = list_first_entry (&rpmsg_dev->active_queue,
+			struct imx_rpmsg_buffer,
+			internal.queue);
 
-		return 0;
+	if (!buf) {
+		spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
+		return -ENOBUFS;
 	}
 
-	buf = list_first_entry(&rpmsg_dev->active_queue, struct imx_rpmsg_buffer,
-				internal.queue);
-
 	vb = &buf->vb.vb2_buf;
-	vb->state = VB2_BUF_STATE_ACTIVE;
+	list_del_init(&buf->internal.queue);
+	to_vb2_v4l2_buffer(vb)->sequence = rpmsg_dev->frame_count++;
+	vb2_buffer_done(vb, VB2_BUF_STATE_DONE);
 
-	dma_addr = vb2_dma_contig_plane_dma_addr(vb, 0);
 	spin_unlock_irqrestore(&rpmsg_dev->slock, flags);
 
-	ret = imx_rpmsg_config_dma(rpmsg_dev, dma_addr, field);
-	return ret;
+	return 0;
 }
 
 static void imx_rpmsg_callback(uint32_t addr, uint32_t buf_num, void *ptr)
 {
 	struct imx_rpmsg_device *rpmsg_dev = ptr;
+/*    static u32 nbr = 0;
+    if((++nbr % 900) == 0) {
+        dev_err(rpmsg_dev->dev, "Received %d rpmsg done callback (streaming: %d).", nbr, rpmsg_dev->streaming);
+    }
+*/
 	if (rpmsg_dev->streaming)
-		imx_rpmsg_dma_done_handle(rpmsg_dev);
+		imx_rpmsg_dma_done_handle(rpmsg_dev, (dma_addr_t)addr);
 }
 
 static int imx_rpmsg_subdev_bound(struct v4l2_async_notifier *notifier,
@@ -960,7 +963,6 @@ static int imx_rpmsg_probe(struct platform_device *pdev)
 	pm_runtime_get_sync(rpmsg_dev->dev);
 
 	INIT_LIST_HEAD(&rpmsg_dev->active_queue);
-	INIT_LIST_HEAD(&rpmsg_dev->discard);
 
 	rpmsg_setup_callback(imx_rpmsg_callback, (void *)rpmsg_dev);
 
-- 
2.17.1

